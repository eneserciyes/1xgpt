{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import lpips\n",
    "import torch\n",
    "import transformers\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "# 1xgpt imports\n",
    "sys.path.append(os.getcwd())\n",
    "from data import RawTokenDataset\n",
    "from visualize import decode_latents_wrapper\n",
    "from eval_utils import decode_tokens, compute_lpips, AvgMetric, compute_loss\n",
    "from genie.st_mask_git import STMaskGIT\n",
    "\n",
    "# Hardcoded values for the v1.1 dataset\n",
    "WINDOW_SIZE = 16\n",
    "STRIDE = 15  # Data is 30 Hz so with stride 15, video is 2 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluate GENIE-style models.\")\n",
    "    parser.add_argument(\n",
    "        \"--val_data_dir\", type=str, default=\"data/val_v1.1\",\n",
    "        help=\"A directory with video data, should have a `metadata.json` and `video.bin`.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint_dir\", type=str,\n",
    "        help=\"Path to a HuggingFace-style checkpoint.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=16,\n",
    "        help=\"Batch size, current script only supports a single GPU.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--maskgit_steps\", type=int, default=2, help=\"Number of MaskGIT sampling steps.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--temperature\", type=float, default=0,\n",
    "        help=\"Sampling temperature. If `temperature` <= 1e-8, will do greedy sampling.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_outputs_dir\", type=str,\n",
    "        help=\"Debug option. If specified, will save model predictions and ground truths to this directory. \"\n",
    "             \"Specifically, will save `{pred_frames,pred_logits,gtruth_frames,gtruth_tokens}.pt`\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_examples\", type=int,\n",
    "        help=\"If specified, will stop evaluation early after `max_examples` examples.\"\n",
    "    )\n",
    "\n",
    "    return parser.parse_args([\"--checkpoint_dir\", \"1x-technologies/GENIE_138M\", \"--maskgit_steps\", \"2\"])\n",
    "\n",
    "args = parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenieEvaluator:\n",
    "    def __init__(self, args, decode_latents, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = STMaskGIT.from_pretrained(args.checkpoint_dir)\n",
    "\n",
    "        self.model = self.model.to(device=device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.decode_latents = decode_latents\n",
    "        self.device = device\n",
    "        self.args = args\n",
    "\n",
    "    def predict_zframe_logits(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor]:\n",
    "        \"\"\"\n",
    "        Conditioned on each prefix: [frame_0], [frame_0, frame_1], ..., [frame_0, frame_1, ... frame_{T-1}],\n",
    "        predict the tokens in the following frame: [pred_frame_1, pred_frame_2, ..., pred_frame_T].\n",
    "\n",
    "        Image logits are denoised in parallel across spatial dimension and teacher-forced\n",
    "        across the time dimension. To compute logits, we save both the samples and logits as we do MaskGIT generation.\n",
    "\n",
    "        Total number of forward passes is (T-1) * maskgit steps.\n",
    "\n",
    "        Args:\n",
    "            input_ids: LongTensor of size (B, T*H*W) corresponding to flattened, tokenized images.\n",
    "\n",
    "        Returns: (samples_THW, factored_logits)\n",
    "            samples_THW:\n",
    "                size (B, T, H, W) corresponding to the token ids of the predicted frames.\n",
    "                May differ from the argmax of `factored_logits` if not greedy sampling.\n",
    "            factored_logits:\n",
    "                size (B, 512, 2, T-1, H, W) corresponding to the predicted logits.\n",
    "                Note that we are factorizing the 2**18 vocabulary into two separate vocabularies of size 512 each.\n",
    "        \"\"\"\n",
    "        inputs_THW = rearrange(input_ids, \"b (t h w) -> b t h w\", t=WINDOW_SIZE,\n",
    "                               h=self.args.latent_h, w=self.args.latent_w).to(self.device)\n",
    "        all_samples = []\n",
    "        all_logits = []\n",
    "        for timestep in range(1, WINDOW_SIZE):\n",
    "            print(f\"Generating frame {timestep}\")\n",
    "            inputs_masked = inputs_THW.clone()\n",
    "            inputs_masked[:, timestep:] = self.model.mask_token_id\n",
    "\n",
    "            # MaskGIT sampling\n",
    "            samples_HW, factored_logits = self.model.maskgit_generate(\n",
    "                inputs_masked, out_t=timestep, maskgit_steps=self.args.maskgit_steps,\n",
    "                temperature=self.args.temperature,\n",
    "            )\n",
    "\n",
    "            all_samples.append(samples_HW)\n",
    "            all_logits.append(factored_logits)\n",
    "\n",
    "        samples_THW = torch.stack(all_samples, dim=1)\n",
    "        return samples_THW, torch.stack(all_logits, dim=3)\n",
    "\n",
    "    def predict_next_frames(self, samples_THW) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        All model submissions should have this defined.\n",
    "\n",
    "        Like predict_next_frames, this is teacher-forced along time dimension, autoregressive along spatial dimension.\n",
    "\n",
    "        Conditioned on each prefix: [frame_0], [frame_0, frame_1], ..., [frame_0, frame_1, ..., frame_{T-1}],\n",
    "        predict the following frame: [pred_frame_1, pred_frame_2, ..., pred_frame_T].\n",
    "\n",
    "        For this model, the frames are generated by using the argmax of `predict_zframe_logits`\n",
    "        and decoding the quantized latent space tokens back to the original image space.\n",
    "\n",
    "        Args:\n",
    "            samples_THW: LongTensor of size (B, T, H, W) corresponding to sampled images in the quantized latent space.\n",
    "\n",
    "        Returns:\n",
    "            LongTensor of size (B, T-1, 3, 256, 256) corresponding to the predicted frames.\n",
    "        \"\"\"\n",
    "        return decode_tokens(samples_THW.cpu(), self.decode_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.set_seed(42)\n",
    "args = parse_args()\n",
    "\n",
    "val_dataset = RawTokenDataset(args.val_data_dir, window_size=WINDOW_SIZE, stride=STRIDE, filter_overlaps=True)\n",
    "args.latent_h = args.latent_w = val_dataset.metadata[\"s\"]\n",
    "\n",
    "decode_latents = decode_latents_wrapper()\n",
    "lpips_alex = lpips.LPIPS(net=\"alex\")  # Calculate LPIPS w/ AlexNet, which is the fastest model out of their options\n",
    "\n",
    "if args.max_examples is not None:\n",
    "    val_dataset.valid_start_inds = val_dataset.valid_start_inds[:args.max_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(val_dataset, collate_fn=default_data_collator, batch_size=args.batch_size)\n",
    "\n",
    "evaluator = GenieEvaluator(args, decode_latents)\n",
    "metrics = defaultdict(AvgMetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.save_outputs_dir is not None:\n",
    "    outputs_to_save = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(batch):\n",
    "    batch_size = batch[\"input_ids\"].size(0)\n",
    "    reshaped_input_ids = rearrange(batch[\"input_ids\"], \"b (t h w) -> b t h w\", t=WINDOW_SIZE,\n",
    "                                    h=args.latent_h, w=args.latent_w)\n",
    "\n",
    "    start_time = time.time()\n",
    "    samples, factored_logits = evaluator.predict_zframe_logits(batch[\"input_ids\"])\n",
    "    frames_per_batch = (WINDOW_SIZE - 1) * batch[\"input_ids\"].size(0)\n",
    "    metrics[\"gen_time\"].update((time.time() - start_time) / frames_per_batch, batch_size)\n",
    "\n",
    "    loss = compute_loss(batch[\"labels\"], factored_logits)\n",
    "\n",
    "    acc = (reshaped_input_ids[:, 1:].to(\"cuda\") == samples).float().mean().item()\n",
    "\n",
    "    metrics[\"loss\"].update(loss, batch_size)\n",
    "    metrics[\"acc\"].update(acc, batch_size)\n",
    "\n",
    "    start_time = time.time()\n",
    "    pred_frames = evaluator.predict_next_frames(samples)\n",
    "    metrics[\"dec_time\"].update((time.time() - start_time) / frames_per_batch, batch_size)\n",
    "\n",
    "    decoded_gtruth = decode_tokens(reshaped_input_ids, decode_latents)\n",
    "    metrics[\"pred_lpips\"].update_list(compute_lpips(decoded_gtruth[:, 1:], pred_frames, lpips_alex))\n",
    "\n",
    "    return samples, decoded_gtruth, factored_logits, batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, gtruth, factored_logits, gt_labels = forward_pass(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factored_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genie.factorization_utils import factorize_labels\n",
    "\n",
    "def compute_loss_here():\n",
    "    labels_flat = gt_labels\n",
    "    num_factored_vocabs = 2\n",
    "    factored_vocab_size = 512\n",
    "\n",
    "    t = factored_logits.size(3) + 1\n",
    "    h, w = factored_logits.size()[-2:]\n",
    "    print(\"t, h, w:\", t, h, w)\n",
    "    assert t * h * w == labels_flat.size(1), \"Shape of `factored_logits` does not match flattened latent image size.\"\n",
    "\n",
    "    labels_THW = rearrange(labels_flat, \"b (t h w) -> b t h w\", t=t, h=h, w=w)\n",
    "    print(labels_THW.shape)\n",
    "    labels_THW = labels_THW[:, 1:].to(factored_logits.device)\n",
    "    print(labels_THW.shape)\n",
    "\n",
    "    factored_labels = factorize_labels(labels_THW, num_factored_vocabs, factored_vocab_size)\n",
    "    print(factored_labels.shape)\n",
    "    print(factored_logits.shape)\n",
    "\n",
    "    loss_unreduced = torch.nn.functional.cross_entropy(factored_logits, factored_labels, reduction=\"none\").sum(dim=1)\n",
    "    print(loss_unreduced.shape)\n",
    "\n",
    "    loss = loss_unreduced.mean().item() # Final loss is the sum of the two losses across the size-512 vocabularies\n",
    "    print(loss)\n",
    "\n",
    "    return loss_unreduced.cpu().detach(), loss\n",
    "\n",
    "loss_viz, loss= compute_loss_here()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown of the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(gtruth[1, 9].permute(1,2,0), label=\"gtruth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for b in range(16):\n",
    "    for i in range(15):\n",
    "        fig= plt.figure(figsize=(10,10))\n",
    "\n",
    "        image = gtruth[b, i+1].permute(1,2,0)\n",
    "        image_gray = np.dot(image.numpy()[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "        errors = np.kron(loss_viz[b,i].cpu().numpy(), np.ones((16, 16)))\n",
    "\n",
    "        plt.imshow(image_gray, cmap='gray', alpha=1)\n",
    "        plt.imshow(errors, alpha=0.3, cmap=\"viridis\")\n",
    "        plt.colorbar()\n",
    "        fig.savefig(f\"error_maps/error_map_batch{b:02d}_frame{i:02d}.png\")\n",
    "        fig.clf()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_viz[0,0].mean())\n",
    "print(errors.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(errors, cmap='viridis',)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factored_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
